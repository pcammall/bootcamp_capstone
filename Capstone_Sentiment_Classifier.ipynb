{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f137c340",
   "metadata": {},
   "source": [
    "# Sentiment Classifier\n",
    "\n",
    "This is a a sentiment classifer using Random Forest. The data sets are pulled from Kaggle:\n",
    "__link_here__\n",
    "\n",
    "\n",
    "First, begin by importing several items needed for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "740f1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebb002",
   "metadata": {},
   "source": [
    "## Data Exploration and Cleansing\n",
    "\n",
    "There's some additional items to import that will help with cleaning the input data. The input is text, and need to undergo some cleansing to reach a more standardized form before it is given to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aefb1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expressions\n",
    "import string\n",
    "import nltk #Natural Language \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46cb72bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PGNX  Over 3.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAP - user if so then the current downtrend wi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Monday's relative weakness. NYX WIN TIE TAP IC...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GOOG - ower trend line channel test &amp; volume s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AAP will watch tomorrow for ONG entry.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i'm assuming FCX opens tomorrow above the 34.2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>It really worries me how everyone expects the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AAP GAMCO's arry Haverty : Apple Is Extremely ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>user Maykiljil posted that.  I agree that MSFT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Momentum is coming back to ETFC Broke MA200 re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HA Hitting 35.65 means resume targeting 42 lev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>user gameplan shot for today but I liked  on t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>with FCX gapping well above ideal entry lookin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>user great list again, particularly like FISV ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ATHX upper trend line</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NG - nice PNF BY - breakout - need follow thru</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Won't believe AAP uptrend is back until it cro...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>X swing still on</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>SWY - 30% of float short and breaking out - ouch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>BIOF wants 4.90's comin!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>VS inverted head and shoulder play out well. W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>red, not ready for break out.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>EI close to breaking out now.  My trigger is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>user BAC For a quick Trade to late..But for in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CHDN - ong   49.02. Trailing Stop  56.66 from ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AAP VOME today is impressive. At this rate and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment\n",
       "0   user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
       "1   user I'd be afraid to short AMZN - they are lo...          1\n",
       "2                                   MNTA Over 12.00            1\n",
       "3                                    OI  Over 21.37            1\n",
       "4                                   PGNX  Over 3.04            1\n",
       "5   AAP - user if so then the current downtrend wi...         -1\n",
       "6   Monday's relative weakness. NYX WIN TIE TAP IC...         -1\n",
       "7   GOOG - ower trend line channel test & volume s...          1\n",
       "8              AAP will watch tomorrow for ONG entry.          1\n",
       "9   i'm assuming FCX opens tomorrow above the 34.2...          1\n",
       "10  It really worries me how everyone expects the ...          1\n",
       "11  AAP GAMCO's arry Haverty : Apple Is Extremely ...          1\n",
       "12  user Maykiljil posted that.  I agree that MSFT...          1\n",
       "13  Momentum is coming back to ETFC Broke MA200 re...          1\n",
       "14  HA Hitting 35.65 means resume targeting 42 lev...          1\n",
       "15  user gameplan shot for today but I liked  on t...          1\n",
       "16  with FCX gapping well above ideal entry lookin...          1\n",
       "17  user great list again, particularly like FISV ...          1\n",
       "18                           ATHX upper trend line             1\n",
       "19   NG - nice PNF BY - breakout - need follow thru            1\n",
       "20  Won't believe AAP uptrend is back until it cro...         -1\n",
       "21                                X swing still on             1\n",
       "22   SWY - 30% of float short and breaking out - ouch          1\n",
       "23                         BIOF wants 4.90's comin!!!          1\n",
       "24  VS inverted head and shoulder play out well. W...          1\n",
       "25                      red, not ready for break out.         -1\n",
       "26  EI close to breaking out now.  My trigger is a...          1\n",
       "27  user BAC For a quick Trade to late..But for in...          1\n",
       "28  CHDN - ong   49.02. Trailing Stop  56.66 from ...          1\n",
       "29  AAP VOME today is impressive. At this rate and...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input_data = pd.read_csv(r'C:\\Users\\Patrick\\Documents\\GitHub\\bootcamp_capstone\\kaggle_dataset\\sentiment_analysis_financial_news\\all-data.csv'\n",
    "#                , encoding = \"ISO-8859-1\", header=None, names=['sentiment', 'text'])\n",
    "\n",
    "\n",
    "input_data = pd.read_csv(r'C:\\Users\\Patrick\\Documents\\GitHub\\bootcamp_capstone\\kaggle_dataset\\stock-market_sentiment\\stock_data.csv',\n",
    "                        encoding=\"ISO-8859-1\", header=1, names=['text', 'sentiment'] )\n",
    "\n",
    "input_data.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5a93f",
   "metadata": {},
   "source": [
    "A quick preview of the data shows that the data lacks a consistent form. Some rows have special characters, while others have none at all; each line is of a different length; there is no consistent form.\n",
    "The cleanup begins by removing special characters, and converting everything to lower case. Then, prefixes and suffixes can be removed (lemmatizing), reducing words to ty and get a level of consistency.\n",
    " \n",
    "\n",
    "Stopwords are also removed. These are sentence modifiers like \"A\", \"The\", \"And\", \"This\". They don't add much information to a sentence, but exist because of grammar rules for human readabilty. They aren't necessary for the ML Model to extract the sentinment. The stopwords here will default to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04235133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "pattern = r'[^a-zA-Z0-9\\s\\%]'\n",
    "cleaned_buffer = []\n",
    "for x in input_data['text']:\n",
    "    temp = re.sub(pattern, \" \", x)\n",
    "    temp = temp.lower()\n",
    "    temp = temp.split()\n",
    "    temp = [lemmatizer.lemmatize(word) for word in temp if not word in set(stopwords)]\n",
    "    temp = ' '.join(temp)\n",
    "    cleaned_buffer.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d861e1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "      <td>user aap movie 55% return fea geed indicator 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>user afraid short amzn looking like near monop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "      <td>mnta 12 00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "      <td>oi 21 37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PGNX  Over 3.04</td>\n",
       "      <td>1</td>\n",
       "      <td>pgnx 3 04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAP - user if so then the current downtrend wi...</td>\n",
       "      <td>-1</td>\n",
       "      <td>aap user current downtrend break otherwise sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Monday's relative weakness. NYX WIN TIE TAP IC...</td>\n",
       "      <td>-1</td>\n",
       "      <td>monday relative weakness nyx win tie tap ice i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GOOG - ower trend line channel test &amp; volume s...</td>\n",
       "      <td>1</td>\n",
       "      <td>goog ower trend line channel test volume support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AAP will watch tomorrow for ONG entry.</td>\n",
       "      <td>1</td>\n",
       "      <td>aap watch tomorrow ong entry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i'm assuming FCX opens tomorrow above the 34.2...</td>\n",
       "      <td>1</td>\n",
       "      <td>assuming fcx open tomorrow 34 25 trigger buy s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  user: AAP MOVIE. 55% return for the FEA/GEED i...          1   \n",
       "1  user I'd be afraid to short AMZN - they are lo...          1   \n",
       "2                                  MNTA Over 12.00            1   \n",
       "3                                   OI  Over 21.37            1   \n",
       "4                                  PGNX  Over 3.04            1   \n",
       "5  AAP - user if so then the current downtrend wi...         -1   \n",
       "6  Monday's relative weakness. NYX WIN TIE TAP IC...         -1   \n",
       "7  GOOG - ower trend line channel test & volume s...          1   \n",
       "8             AAP will watch tomorrow for ONG entry.          1   \n",
       "9  i'm assuming FCX opens tomorrow above the 34.2...          1   \n",
       "\n",
       "                                             cleaned  \n",
       "0  user aap movie 55% return fea geed indicator 1...  \n",
       "1  user afraid short amzn looking like near monop...  \n",
       "2                                         mnta 12 00  \n",
       "3                                           oi 21 37  \n",
       "4                                          pgnx 3 04  \n",
       "5  aap user current downtrend break otherwise sho...  \n",
       "6  monday relative weakness nyx win tie tap ice i...  \n",
       "7   goog ower trend line channel test volume support  \n",
       "8                       aap watch tomorrow ong entry  \n",
       "9  assuming fcx open tomorrow 34 25 trigger buy s...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data['cleaned'] = cleaned_buffer\n",
    "input_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe4ea5",
   "metadata": {},
   "source": [
    "In some cases, the data cleansing has removed more information that necessary, making the resulting statement rather meaningless (rows 2, 3, and 4 above). With the additional rows of data, this can be overcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac458b",
   "metadata": {},
   "source": [
    "## Creating Training and Test Data\n",
    "\n",
    "Now that the data is cleansed, the next step is to split it into a training and a test set. The data must also be converted into a number format, as the Machine Learning model cannot comprehend text. \n",
    "\n",
    "Converting the input data into numbers is done with a TFIDF Vectorizer. This will look at the words in the given input, and generate a mapping of which word(s) go together and how often. It's set to do up to 3 words at a time. The TFIDF is extracting features that the ML Model will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ced720d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split( input_data['cleaned'], input_data['sentiment'],\n",
    "                                                               test_size=.4, random_state=10)\n",
    "#60% of the data will be used for training. 40% to test.\n",
    "#the random state number is so that each time this is run it generates the same result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002c76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "xtrain_tf = tfidf.fit_transform(xtrain)\n",
    "xtest_tf = tfidf.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ae9a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples: 2316, nfeatures: 57630\n",
      "  (0, 45368)\t0.510097570366633\n",
      "  (0, 25696)\t0.510097570366633\n",
      "  (0, 25695)\t0.510097570366633\n",
      "  (0, 14855)\t0.3210467802934407\n",
      "  (0, 12960)\t0.3410723837858898\n",
      "  (1, 31576)\t0.822655797157474\n",
      "  (1, 31525)\t0.4690720683613014\n",
      "  (1, 5270)\t0.32126131744492964\n"
     ]
    }
   ],
   "source": [
    "print(\"nsamples: %d, nfeatures: %d\" % xtest_tf.shape)\n",
    "print(xtest_tf[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdcf521",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fccd9d2a",
   "metadata": {},
   "source": [
    "Now for actually building and training the model. The model is a Random Forest Classifier from SciKitLearn.\n",
    "\n",
    "The amount of CrossValidation done here is arbitrary, and can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05b24811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76546763 0.74244604 0.75395683 0.73093525 0.72622478]\n"
     ]
    }
   ],
   "source": [
    "rand_forest = RandomForestClassifier()\n",
    "scores = cross_val_score(rand_forest, xtrain_tf, ytrain, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aec4d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 2, 'n_estimators': 5} \t 0.6309729853005204 \t 0.0006478831006395653\n",
      "{'max_depth': 2, 'n_estimators': 10} \t 0.6315485248688658 \t 0.00021230278025421434\n",
      "{'max_depth': 2, 'n_estimators': 25} \t 0.6315485248688658 \t 0.00021230278025421434\n",
      "{'max_depth': 2, 'n_estimators': 50} \t 0.6315485248688658 \t 0.00021230278025421434\n",
      "{'max_depth': 2, 'n_estimators': 100} \t 0.6315485248688658 \t 0.00021230278025421434\n",
      "{'max_depth': 5, 'n_estimators': 5} \t 0.6321244790910787 \t 0.0016862283817811348\n",
      "{'max_depth': 5, 'n_estimators': 10} \t 0.6327008479671594 \t 0.001481704893706652\n",
      "{'max_depth': 5, 'n_estimators': 25} \t 0.6315485248688658 \t 0.00021230278025421434\n",
      "{'max_depth': 5, 'n_estimators': 50} \t 0.6315485248688658 \t 0.00021230278025421434\n",
      "{'max_depth': 5, 'n_estimators': 100} \t 0.6315485248688658 \t 0.00021230278025421434\n",
      "{'max_depth': 10, 'n_estimators': 5} \t 0.6416221259303796 \t 0.009419443001572986\n",
      "{'max_depth': 10, 'n_estimators': 10} \t 0.6370173947297493 \t 0.003610886162135585\n",
      "{'max_depth': 10, 'n_estimators': 25} \t 0.6318362946530384 \t 0.0006613717490850005\n",
      "{'max_depth': 10, 'n_estimators': 50} \t 0.6347148218025004 \t 0.0019361538503012957\n",
      "{'max_depth': 10, 'n_estimators': 100} \t 0.6326996040055564 \t 0.0019832179692504657\n",
      "{'max_depth': 20, 'n_estimators': 5} \t 0.6706918499782306 \t 0.01695940981663213\n",
      "{'max_depth': 20, 'n_estimators': 10} \t 0.6537134327120437 \t 0.008411562682326904\n",
      "{'max_depth': 20, 'n_estimators': 25} \t 0.6473791802293036 \t 0.006997193480748322\n",
      "{'max_depth': 20, 'n_estimators': 50} \t 0.6485315033275973 \t 0.002530455727604109\n",
      "{'max_depth': 20, 'n_estimators': 100} \t 0.648530674019862 \t 0.005103125523654623\n",
      "{'max_depth': None, 'n_estimators': 5} \t 0.7291306781664006 \t 0.008457454309629261\n",
      "{'max_depth': None, 'n_estimators': 10} \t 0.7389189973669479 \t 0.008228116024238513\n",
      "{'max_depth': None, 'n_estimators': 25} \t 0.735169282441482 \t 0.016639316788419782\n",
      "{'max_depth': None, 'n_estimators': 50} \t 0.7458204963406796 \t 0.017535042741996396\n",
      "{'max_depth': None, 'n_estimators': 100} \t 0.7464010117554372 \t 0.013370862650097666\n"
     ]
    }
   ],
   "source": [
    "#the first version of hyperparameter tuning\n",
    "params = { 'n_estimators' : [5, 10, 25, 50, 100], 'max_depth' : [2, 5, 10, 20, None]}\n",
    "\n",
    "grid_search = GridSearchCV (rand_forest, params)\n",
    "grid_search.fit(xtrain_tf, ytrain.values)\n",
    "\n",
    "all_means = grid_search.cv_results_['mean_test_score']\n",
    "all_std_dev = grid_search.cv_results_['std_test_score']\n",
    "all_params = grid_search.cv_results_['params']\n",
    "for x in range(0, len(all_means)):\n",
    "    print(all_params[x], \"\\t\", all_means[x], \"\\t\", all_std_dev[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6b4dd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf2df6",
   "metadata": {},
   "source": [
    "After performing the grid search across every combination of depth and estimators, and printing the results, there is a clear jump in accuracy when there are no constraints on depth. While standard deviation trends downards with more esitmators, there isn't much of a difference between 50 and 100 estimators. Diminishing returns is very strong at that point.\n",
    "\n",
    "\n",
    "For fun, do a search again, with even more input parameters, except this time it uses Random Search. It will take a while, so this is a great time to take a break and remember to eat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da0d0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start=5, stop=300, num=5)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num=5)]\n",
    "max_depth.append(None)\n",
    "\n",
    "min_samples_split = [1,2,4,10,20,25,50,100]\n",
    "min_samples_leaf = [1,2,3,4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "grid = {'n_estimators':n_estimators,\n",
    "        'max_features':max_features,\n",
    "        'max_depth':max_depth,\n",
    "        'min_samples_split':min_samples_split,\n",
    "        'min_samples_leaf':min_samples_leaf,\n",
    "        'bootstrap':bootstrap\n",
    "       }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "678de6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "70 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Patrick\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.70667842        nan 0.72682935 0.68854394        nan 0.68450853\n",
      " 0.69861464 0.66206083 0.66724152 0.69142288        nan 0.7130052\n",
      "        nan 0.71589037        nan 0.6819186  0.7158829  0.70264591\n",
      " 0.70236228 0.7049518         nan 0.74150436 0.66177306 0.69775299\n",
      " 0.68134431 0.6361545  0.75589783 0.6568789         nan        nan\n",
      "        nan 0.63183629 0.68710468 0.63385068 0.74783737 0.70436921\n",
      " 0.66522671 0.72452678 0.68711048 0.71761906 0.65630378 0.63241183\n",
      " 0.73747808 0.68710136 0.65803039 0.70235689 0.63212406 0.68047975\n",
      " 0.7357469  0.70840586 0.73143325 0.75935148 0.63241225 0.66148529\n",
      " 0.69976323 0.68969212 0.66234777 0.71646383 0.70178052        nan\n",
      " 0.68595153 0.63270002 0.66637862 0.66925798 0.65975701        nan\n",
      " 0.68364812 0.69343561 0.63270043 0.65658989 0.70351336        nan\n",
      " 0.70379906 0.66522547 0.72222254 0.6963158  0.63356291 0.63298779\n",
      " 0.67875189        nan 0.65918272 0.71992039 0.63356291 0.73574607\n",
      " 0.71732963 0.69199635 0.66062198 0.66580101 0.63356333 0.67530197\n",
      " 0.66695499        nan 0.63413928 0.70091929 0.70379906 0.72509983\n",
      " 0.73315572 0.68998196 0.6908432  0.65975701]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 32, 55, 77, 100,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 3, 4],\n",
       "                                        'min_samples_split': [1, 2, 4, 10, 20,\n",
       "                                                              25, 50, 100],\n",
       "                                        'n_estimators': [5, 78, 152, 226, 300]},\n",
       "                   random_state=10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Whereas grid search does X1, X2, X3, Y1, Y2, Y3, Z1, Z2.... Random search instead picks\n",
    "#points within the \"grid\" of parameters to test\n",
    "#this will take some time, so use n_jobs=-1 to use all the proceessors on the computer\n",
    "#DO NOT RUN WITH n_iter=1000 (that takes a solid 7 hours)\n",
    "rf_rand = RandomizedSearchCV(estimator=rand_forest, param_distributions=grid, n_iter=100, \n",
    "                            cv=5, random_state=10,n_jobs=-1)\n",
    "rf_rand.fit(xtrain_tf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c76b610f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, min_samples_leaf=2, min_samples_split=4,\n",
       "                       n_estimators=152)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d7e4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now create a RandForestClassifier using the best estimator from the random search.\n",
    "forest = RandomForestClassifier(bootstrap=False, max_features='sqrt', min_samples_split=5, n_estimators=152)\n",
    "forest.fit(xtrain_tf, ytrain)\n",
    "forest_predictions = forest.predict(xtest_tf)\n",
    "results = metrics.classification_report(ytest, forest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fcabb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.64      0.65      0.64       826\n",
      "           1       0.80      0.80      0.80      1490\n",
      "\n",
      "    accuracy                           0.74      2316\n",
      "   macro avg       0.72      0.72      0.72      2316\n",
      "weighted avg       0.75      0.74      0.75      2316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49cc9e",
   "metadata": {},
   "source": [
    "This is just basic hyperparameter tuning to find the best parameters for the constructor. More in-depth tuning has to take place after using the model for some time to understand what other areas need to be optimized. \n",
    "\n",
    "Now the model for production is ready to create using the best estimators from the search. This will be saved to a Pickle file, avoiding the need to train the file every time a prediction is desired, and also avoiding the need dto store or provide the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49f5eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = \"my_saved_model\"\n",
    "pickle.dump(forest, open(filename, 'wb+'))\n",
    "\n",
    "filename = \"my_saved_tfidf\"\n",
    "pickle.dump(tfidf, open(filename, 'wb+'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
